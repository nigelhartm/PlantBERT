
Training . . .
  0%|                                                                                                                                                                                                                        | 0/42735 [00:00<?, ?it/s]/scratch/users/nigel.hartman/bin/miniconda3/envs/plantbert/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
MaskedLMOutput(loss=tensor([8.4709, 8.4771, 8.4786, 8.4780], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 4.7365e-01,  0.0000e+00, -2.4118e-01,  ...,  7.7278e-01,
          -1.2152e+00, -3.1626e-01],
         [ 5.4431e-02,  0.0000e+00, -2.5148e-01,  ..., -7.2285e-02,
           8.6014e-01,  5.9768e-01],
         [-1.0659e-01,  0.0000e+00, -3.9702e-01,  ...,  4.9300e-01,
          -2.6387e-01, -4.8392e-01],
         ...,
         [-5.5901e-01,  0.0000e+00, -1.2534e-01,  ...,  1.8089e-01,
          -2.1557e-01, -1.0377e-01],
         [-4.8200e-01,  0.0000e+00, -5.3491e-01,  ...,  1.1092e-01,
           3.7030e-01, -1.2787e-01],
         [-1.8166e-01,  0.0000e+00, -5.4468e-01,  ..., -2.8057e-01,
           1.1856e-02,  4.8288e-01]],
        [[ 8.6572e-02,  0.0000e+00, -3.7755e-01,  ...,  9.1411e-01,
          -1.0044e+00, -6.3974e-01],
         [ 8.6656e-02,  0.0000e+00,  1.4969e-01,  ..., -1.8144e-01,
           4.6365e-01, -6.0917e-01],
         [-8.4316e-01,  0.0000e+00, -5.1539e-01,  ..., -4.9785e-01,
          -5.1415e-02, -8.1352e-01],
         ...,
         [-1.1710e-02,  0.0000e+00, -1.0298e-01,  ...,  1.6085e-01,
          -2.6222e-01,  3.4826e-01],
         [-2.1421e-01,  0.0000e+00, -1.2058e-01,  ...,  4.6618e-02,
          -3.3569e-03, -1.2609e-01],
         [-3.6001e-01,  0.0000e+00, -2.1554e-01,  ...,  1.3323e-01,
          -2.2624e-01,  7.5251e-02]],
        [[ 4.5131e-01,  0.0000e+00, -3.4000e-01,  ...,  9.7231e-01,
          -7.9746e-01, -3.6363e-01],
         [ 1.5305e-03,  0.0000e+00, -3.0933e-01,  ..., -2.9408e-01,
           6.9181e-01,  1.6410e-01],
         [ 4.4064e-02,  0.0000e+00, -6.4309e-01,  ..., -1.9116e-01,
           4.7174e-02,  2.3526e-01],
         ...,
         [ 7.3378e-02,  0.0000e+00, -2.3788e-01,  ..., -1.8902e-02,
           4.8962e-01,  2.1094e-01],
         [-4.6718e-01,  0.0000e+00, -5.1671e-01,  ..., -2.8962e-01,
           1.2422e-02,  3.9320e-02],
         [-5.1928e-01,  0.0000e+00, -1.0230e-01,  ...,  4.6684e-01,
          -1.3795e-01, -9.2321e-02]],
        ...,
        [[ 4.8752e-01,  0.0000e+00, -1.7121e-01,  ...,  6.0659e-01,
          -6.0179e-01, -5.9009e-01],
         [-3.7281e-01,  0.0000e+00,  5.3030e-01,  ..., -6.4298e-01,
           4.0378e-01, -4.8044e-01],
         [-3.3080e-02,  0.0000e+00, -3.7436e-01,  ..., -1.0012e-01,
          -1.4453e-01,  1.0969e-01],
         ...,
         [-7.5380e-01,  0.0000e+00,  3.0452e-02,  ...,  1.7742e-01,
          -3.2076e-01,  3.4895e-02],
         [-4.5592e-01,  0.0000e+00, -9.9021e-02,  ..., -6.7945e-02,
          -1.8820e-02, -1.5938e-01],
         [-9.5548e-01,  0.0000e+00, -5.3150e-01,  ...,  2.5577e-01,
          -1.4653e-01, -2.5625e-01]],
        [[ 4.1073e-01,  0.0000e+00, -5.3550e-01,  ...,  6.4337e-01,
          -2.4735e-01,  7.8266e-02],
         [-3.0664e-01,  0.0000e+00, -1.2571e-01,  ...,  4.5161e-01,
           7.2321e-01, -1.4038e-01],
         [ 2.9801e-03,  0.0000e+00, -1.8718e-01,  ..., -1.0298e-02,
           6.7612e-02, -4.7931e-01],
         ...,
         [-4.4889e-01,  0.0000e+00, -4.0376e-01,  ..., -6.5554e-04,
           3.1216e-01,  9.3442e-03],
         [-3.0181e-01,  0.0000e+00, -3.1908e-01,  ..., -3.1024e-01,
           1.0387e-01,  4.8594e-01],
         [-3.8451e-01,  0.0000e+00, -8.8807e-02,  ...,  9.9714e-02,
          -2.4816e-01,  5.3316e-01]],
        [[ 4.4254e-01,  0.0000e+00, -3.3401e-01,  ...,  1.1116e+00,
          -1.0306e+00, -2.6166e-01],
         [ 1.3351e-01,  0.0000e+00,  5.8822e-01,  ..., -2.4229e-01,
          -1.0672e-01,  5.9665e-02],
         [ 7.4990e-02,  0.0000e+00, -1.3809e-01,  ...,  2.6031e-01,
           2.7083e-01, -5.4053e-02],
         ...,
         [-4.1585e-01,  0.0000e+00, -5.1118e-01,  ...,  2.7797e-01,
           4.9227e-01, -1.2753e-01],
         [-3.3266e-01,  0.0000e+00,  5.8419e-02,  ..., -7.0323e-02,
          -1.7064e-01,  5.9645e-01],
         [-6.0612e-01,  0.0000e+00,  5.2325e-01,  ...,  4.4394e-01,
          -4.0470e-01,  1.4895e-01]]], device='cuda:0',
       grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
MaskedLMOutput(loss=tensor([8.3168, 8.3196, 8.3194, 8.3146], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 2.8003,  0.0197,  0.4986,  ...,  0.4084, -0.7233, -0.7177],
         [ 1.2338,  0.0199,  0.8759,  ...,  0.0749,  0.1906, -0.9766],
         [ 1.3392,  0.0219,  0.1562,  ..., -0.1036,  0.1305, -0.6797],
         ...,
         [ 1.6603,  0.0282,  1.0084,  ...,  0.4323, -0.0327, -0.3214],
         [ 1.6288,  0.0290,  1.0890,  ...,  0.4929,  0.1682, -0.8022],
         [ 1.6326,  0.0292,  1.0048,  ...,  0.4662, -0.0660, -0.7084]],
        [[ 3.3444,  0.0188,  0.7054,  ...,  0.1649, -0.5962, -0.6784],
         [ 2.1820,  0.0204,  0.7884,  ...,  0.1020, -0.0059, -0.1955],
         [ 1.9260,  0.0194,  0.3315,  ..., -0.1254, -0.1152, -0.7725],
         ...,
         [ 1.9079,  0.0287,  0.5438,  ...,  0.0075, -0.1147, -0.7073],
         [ 1.3750,  0.0276,  0.8658,  ...,  0.3574,  0.0562, -1.0694],
         [ 2.0426,  0.0278,  0.8108,  ...,  0.2633, -0.3812, -0.7207]],
        [[ 3.1539,  0.0202,  0.5347,  ..., -0.0269, -0.6537, -0.5120],
         [ 2.0692,  0.0200,  1.2527,  ...,  0.3596,  0.2827, -0.2921],
         [ 1.9395,  0.0194,  0.7177,  ...,  0.3669, -0.0887, -0.8627],
         ...,
         [ 1.5844,  0.0297,  0.7688,  ...,  0.1763, -0.3377, -0.9234],
         [ 1.8991,  0.0280,  0.6126,  ..., -0.2863,  0.0300, -0.5656],
         [ 1.5181,  0.0277,  0.6787,  ...,  0.7536, -0.1680, -0.7554]],
        ...,
        [[ 3.0595,  0.0212,  0.4623,  ...,  0.2653, -0.7814, -0.6111],
         [ 2.0815,  0.0221,  0.6348,  ...,  0.0527,  0.7403, -0.4336],
         [ 1.0022,  0.0232,  0.9503,  ...,  0.6481, -0.0685, -1.5167],
         ...,
         [ 1.7557,  0.0299,  0.7385,  ...,  0.2133, -0.2599, -0.5154],
         [ 1.7807,  0.0288,  0.9193,  ...,  0.0341,  0.0693, -0.7917],
         [ 1.5307,  0.0271,  0.7065,  ...,  0.3019, -0.1626, -0.7389]],
        [[ 3.2939,  0.0199,  0.3537,  ...,  0.1965, -0.4252, -0.4208],
         [ 2.4268,  0.0198,  0.7860,  ...,  0.0263,  0.0837, -0.0881],
         [ 2.0265,  0.0174,  0.6807,  ...,  0.1073, -0.2114, -1.0691],
         ...,
         [ 1.7929,  0.0286,  0.7136,  ...,  0.7444, -0.2918, -0.8892],
         [ 1.6416,  0.0274,  0.8555,  ...,  0.4686, -0.1356, -0.4768],
         [ 1.6233,  0.0276,  0.7908,  ...,  0.3482,  0.1085, -0.6692]],
        [[ 2.9983,  0.0187,  0.6987,  ...,  0.4468, -0.6294, -0.9398],
         [ 2.0050,  0.0194,  0.7856,  ..., -0.2435,  0.1753, -0.6384],
         [ 1.4714,  0.0217,  0.4508,  ...,  0.1530, -0.0834, -0.5026],
         ...,
         [ 1.6878,  0.0298,  0.8110,  ...,  0.3516, -0.2424, -0.8534],
         [ 1.7389,  0.0284,  1.3151,  ...,  0.6025,  0.0212, -0.6516],
         [ 1.7647,  0.0301,  0.3848,  ...,  0.3297,  0.0068, -0.4862]]],
       device='cuda:0', grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
MaskedLMOutput(loss=tensor([8.1888, 8.1929, 8.1869, 8.1909], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 3.4589e+00,  6.2944e-02,  9.4806e-01,  ...,  3.4244e-01,
          -7.6474e-01, -1.6880e-01],
         [ 1.7056e+00,  6.0797e-02,  8.2453e-01,  ..., -1.5576e-01,
           8.0117e-02, -6.4655e-01],
         [ 1.7126e+00,  6.6044e-02,  1.2425e+00,  ...,  2.8478e-01,
          -7.9893e-02, -2.7113e-01],
         ...,
         [ 1.4925e+00,  7.5059e-02,  1.0526e+00,  ...,  2.8587e-01,
          -1.8257e-01, -1.8135e-01],
         [ 1.8043e+00,  7.4689e-02,  1.1219e+00,  ...,  2.5822e-01,
           1.8941e-01, -1.5083e-01],
         [ 1.4062e+00,  7.5283e-02,  1.1165e+00,  ...,  1.2039e-01,
          -7.3606e-02, -7.0364e-02]],
        [[ 3.5163e+00,  6.3875e-02,  7.6158e-01,  ...,  1.6658e-01,
          -8.7211e-01, -1.0845e-01],
         [ 1.7657e+00,  5.9455e-02,  9.1056e-01,  ..., -4.3131e-01,
           1.6715e-01,  3.9296e-01],
         [ 1.8597e+00,  6.3959e-02,  1.0822e+00,  ...,  6.1162e-02,
          -1.1529e-01, -9.1564e-01],
         ...,
         [ 1.3847e+00,  7.7793e-02,  8.2912e-01,  ...,  4.1156e-01,
           5.1823e-03, -2.5993e-01],
         [ 1.6414e+00,  7.4927e-02,  1.1662e+00,  ...,  5.0731e-01,
           8.2340e-02, -9.7799e-02],
         [ 1.4973e+00,  7.8192e-02,  1.1010e+00,  ...,  5.9254e-01,
          -1.9233e-01, -1.2692e-01]],
        [[ 3.7908e+00,  6.0929e-02,  8.8053e-01,  ..., -1.2214e-01,
          -6.0290e-01, -1.9506e-01],
         [ 1.4155e+00,  6.1548e-02,  1.3025e+00,  ..., -4.0399e-01,
           2.7227e-01, -4.2965e-01],
         [ 1.6231e+00,  6.3578e-02,  1.0735e+00,  ...,  4.3642e-01,
          -2.9569e-01, -2.9129e-01],
         ...,
         [ 1.7321e+00,  7.5494e-02,  9.2821e-01,  ...,  7.9157e-02,
          -1.3894e-01, -3.8505e-01],
         [ 1.7648e+00,  7.4532e-02,  1.0973e+00,  ...,  3.0368e-01,
           1.4722e-01, -1.0716e-01],
         [ 1.4791e+00,  7.5895e-02,  1.1235e+00,  ...,  3.8849e-01,
           6.8804e-02, -2.1428e-03]],
        ...,
        [[ 3.4715e+00,  6.2626e-02,  8.0183e-01,  ...,  1.1016e-01,
          -8.6699e-01, -1.8127e-01],
         [ 1.7452e+00,  5.9683e-02,  1.5338e+00,  ...,  8.8073e-02,
           2.3813e-01, -5.3613e-01],
         [ 2.2094e+00,  6.4798e-02,  8.7658e-01,  ...,  1.1058e-01,
          -5.2696e-01, -5.8654e-01],
         ...,
         [ 1.6147e+00,  7.4986e-02,  1.0708e+00,  ...,  4.7450e-01,
          -7.8212e-02, -5.4848e-02],
         [ 1.4591e+00,  7.7134e-02,  1.0372e+00,  ...,  3.7049e-01,
          -1.1489e-01, -1.2611e-02],
         [ 1.5499e+00,  7.6218e-02,  1.0442e+00,  ...,  4.7567e-01,
           2.0830e-01, -4.2603e-01]],
        [[ 3.7653e+00,  6.3430e-02,  1.1210e+00,  ...,  8.7296e-02,
          -8.4977e-01, -3.4558e-02],
         [ 1.8456e+00,  6.1810e-02,  8.5707e-01,  ..., -2.2948e-01,
           1.5146e-01,  1.4261e-01],
         [ 1.4811e+00,  6.8188e-02,  1.1493e+00,  ...,  4.4017e-01,
           1.2241e-01, -1.7966e-01],
         ...,
         [ 1.6811e+00,  7.5258e-02,  1.1827e+00,  ...,  2.7613e-01,
          -1.2541e-01, -2.2665e-01],
         [ 1.7358e+00,  7.5342e-02,  1.1717e+00,  ...,  3.6925e-01,
          -1.9353e-01, -1.9981e-01],
         [ 1.8328e+00,  7.7593e-02,  9.2336e-01,  ...,  1.3312e-01,
           3.6639e-02, -2.9809e-01]],
        [[ 3.7034e+00,  6.3333e-02,  1.0285e+00,  ..., -3.9660e-02,
          -7.4784e-01, -1.7889e-01],
         [ 1.5750e+00,  6.3251e-02,  1.0846e+00,  ..., -1.7376e-01,
           1.6515e-01, -4.3219e-01],
         [ 1.9503e+00,  6.2487e-02,  7.9427e-01,  ..., -2.0047e-01,
          -4.0524e-01,  2.1233e-01],
         ...,
         [ 1.4400e+00,  7.5526e-02,  1.1357e+00,  ...,  3.5529e-01,
          -7.4127e-02, -2.3980e-01],
         [ 1.7076e+00,  7.3444e-02,  1.1626e+00,  ...,  2.5555e-01,
           1.6489e-02, -3.8652e-01],
         [ 1.6010e+00,  7.6048e-02,  1.1641e+00,  ...,  4.4989e-01,
           1.7014e-01, -1.6827e-01]]], device='cuda:0',
       grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
Epoch 0:   0%|                                                                                                                                                                                         | 5/42735 [00:15<18:05:41,  1.52s/it, loss=31.8]
MaskedLMOutput(loss=tensor([8.0804, 8.0780, 8.0788, 8.0817], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 3.8648e+00,  1.1652e-01,  1.1217e+00,  ...,  1.5019e-01,
          -6.7395e-01,  1.6467e-01],
         [ 1.0615e+00,  1.1415e-01,  1.1438e+00,  ...,  4.5052e-01,
          -1.4869e-01,  9.6103e-02],
         [ 1.4028e+00,  1.1595e-01,  1.3148e+00,  ...,  1.6902e-01,
          -3.2510e-01, -3.3707e-01],
         ...,
         [ 1.3425e+00,  1.2849e-01,  1.0127e+00,  ...,  1.6654e-02,
           7.9019e-02,  2.6743e-01],
         [ 1.1397e+00,  1.3061e-01,  1.1405e+00,  ...,  7.7984e-01,
           5.1309e-02, -1.2499e-01],
         [ 1.2576e+00,  1.3063e-01,  1.0309e+00,  ...,  3.0504e-01,
          -2.0403e-01,  2.4073e-01]],
        [[ 3.5558e+00,  1.1756e-01,  1.1174e+00,  ...,  1.9565e-02,
          -5.8009e-01,  1.9160e-01],
         [ 1.5050e+00,  1.1117e-01,  1.1768e+00,  ..., -5.7125e-02,
          -2.9669e-01,  5.6940e-01],
         [ 1.5147e+00,  1.2006e-01,  1.1982e+00,  ...,  1.3552e-01,
          -1.0066e-01,  5.7127e-02],
         ...,
         [ 1.1700e+00,  1.3070e-01,  8.5888e-01,  ...,  4.1722e-01,
           8.2292e-02,  3.4221e-02],
         [ 1.0266e+00,  1.3016e-01,  1.0113e+00,  ...,  3.3550e-01,
          -7.2144e-02, -1.0537e-01],
         [ 1.1146e+00,  1.2901e-01,  1.0126e+00,  ...,  3.2393e-01,
          -3.5333e-02,  2.1388e-01]],
        [[ 3.8867e+00,  1.1394e-01,  1.2546e+00,  ...,  8.2978e-02,
          -5.8774e-01,  2.4779e-01],
         [ 1.3803e+00,  1.1349e-01,  1.0857e+00,  ..., -4.0316e-02,
          -9.1193e-02,  4.9958e-01],
         [ 1.4434e+00,  1.1789e-01,  1.2957e+00,  ...,  6.3538e-02,
           1.0412e-02, -5.8305e-01],
         ...,
         [ 1.3992e+00,  1.3182e-01,  1.0091e+00,  ...,  4.0241e-01,
           4.4956e-02, -1.8385e-01],
         [ 9.6471e-01,  1.3194e-01,  1.0934e+00,  ...,  4.0317e-01,
           7.0338e-02,  8.3971e-02],
         [ 1.0406e+00,  1.2983e-01,  1.1218e+00,  ...,  4.2575e-01,
           1.4083e-01,  5.6313e-02]],
        ...,
        [[ 3.4914e+00,  1.1815e-01,  8.5710e-01,  ...,  2.2479e-01,
          -5.0510e-01,  1.1246e-01],
         [ 1.2500e+00,  1.1986e-01,  1.0745e+00,  ...,  3.5250e-01,
          -7.0163e-02,  2.5421e-01],
         [ 1.0277e+00,  1.1947e-01,  1.0357e+00,  ...,  2.2275e-01,
          -7.3530e-04, -2.0375e-01],
         ...,
         [ 6.5999e-01,  1.3039e-01,  9.7017e-01,  ...,  4.2687e-01,
           1.3760e-01,  8.6229e-02],
         [ 1.0524e+00,  1.2824e-01,  1.0751e+00,  ...,  4.7831e-01,
          -1.3556e-01,  1.5340e-01],
         [ 1.1226e+00,  1.3060e-01,  9.6032e-01,  ...,  2.3679e-01,
           8.4320e-03,  3.2342e-01]],
        [[ 3.6279e+00,  1.1614e-01,  1.2993e+00,  ...,  2.3633e-01,
          -6.1145e-01, -7.0660e-03],
         [ 1.6102e+00,  1.1739e-01,  1.0433e+00,  ...,  3.0637e-01,
           7.6945e-02,  3.4657e-01],
         [ 1.4913e+00,  1.1537e-01,  8.0909e-01,  ..., -3.1526e-02,
          -2.7599e-02, -2.2293e-01],
         ...,
         [ 1.0683e+00,  1.2922e-01,  1.1236e+00,  ...,  5.1477e-01,
           1.9026e-01,  4.0172e-02],
         [ 9.9447e-01,  1.3163e-01,  1.2139e+00,  ...,  4.4037e-01,
           1.9877e-01,  2.2807e-01],
         [ 1.2236e+00,  1.2931e-01,  1.3189e+00,  ...,  7.3304e-01,
           3.2440e-02,  7.8559e-02]],
        [[ 3.9631e+00,  1.0813e-01,  9.4892e-01,  ..., -1.0068e-01,
          -9.9072e-01,  4.2244e-02],
         [ 1.1668e+00,  1.1098e-01,  1.0583e+00,  ...,  2.5067e-03,
           5.9462e-02, -1.2969e-01],
         [ 1.0617e+00,  1.1719e-01,  9.5040e-01,  ...,  4.6281e-01,
          -1.5904e-01, -1.5864e-01],
         ...,
         [ 1.2019e+00,  1.2928e-01,  1.0375e+00,  ...,  3.3722e-01,
           1.8272e-01,  3.6454e-02],
         [ 1.0692e+00,  1.2875e-01,  9.5978e-01,  ...,  5.1787e-01,
           7.5307e-02,  1.1484e-01],
         [ 1.1335e+00,  1.2999e-01,  9.2563e-01,  ...,  4.5637e-01,
          -1.7293e-01,  2.1150e-01]]], device='cuda:0',
       grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
MaskedLMOutput(loss=tensor([7.9641, 7.9623, 7.9621, 7.9576], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 3.9073e+00,  1.7072e-01,  1.2359e+00,  ..., -2.1870e-01,
          -8.6221e-01,  3.3922e-02],
         [ 1.6197e+00,  1.7567e-01,  9.9808e-01,  ..., -1.3760e-01,
          -2.3532e-01,  2.2109e-01],
         [ 1.5099e+00,  1.7632e-01,  1.2778e+00,  ...,  3.0051e-01,
          -3.0822e-01, -3.8306e-01],
         ...,
         [ 1.2597e+00,  1.9456e-01,  1.0171e+00,  ...,  4.7662e-01,
           6.5812e-02, -2.0155e-01],
         [ 1.2386e+00,  1.9536e-01,  6.8728e-01,  ...,  2.7811e-01,
          -3.0921e-01, -3.5791e-02],
         [ 1.0578e+00,  1.9348e-01,  9.0457e-01,  ...,  1.5412e-01,
          -1.6199e-01,  1.0723e-01]],
        [[ 4.1007e+00,  1.6865e-01,  1.3390e+00,  ...,  7.6512e-03,
          -9.5038e-01,  9.7285e-02],
         [ 1.8208e+00,  1.7691e-01,  1.2739e+00,  ...,  1.9076e-01,
          -5.9360e-01,  2.7276e-01],
         [ 1.8877e+00,  1.7451e-01,  1.0640e+00,  ..., -3.4263e-01,
          -4.1344e-01, -2.7465e-02],
         ...,
         [ 1.1489e+00,  1.8988e-01,  9.1215e-01,  ...,  5.0299e-01,
          -1.5139e-01,  4.9268e-02],
         [ 1.0019e+00,  1.9251e-01,  9.1548e-01,  ...,  2.9456e-01,
          -2.5135e-01, -2.3628e-02],
         [ 1.4297e+00,  1.9278e-01,  1.1846e+00,  ...,  3.6955e-01,
          -4.4858e-01, -1.8477e-02]],
        [[ 3.8629e+00,  1.6810e-01,  1.0138e+00,  ..., -2.8718e-01,
          -7.6133e-01, -3.3889e-03],
         [ 1.5366e+00,  1.7199e-01,  1.5172e+00,  ..., -1.4291e-02,
          -3.2490e-01,  1.4543e-01],
         [ 1.7593e+00,  1.7925e-01,  9.2133e-01,  ...,  5.3192e-02,
          -4.8880e-01, -3.2401e-01],
         ...,
         [ 1.0912e+00,  1.9551e-01,  7.9925e-01,  ...,  3.3477e-01,
          -2.7222e-01, -8.9023e-02],
         [ 1.2529e+00,  1.9383e-01,  1.0242e+00,  ...,  4.9530e-02,
          -2.3261e-01,  5.8604e-02],
         [ 1.1150e+00,  1.9418e-01,  1.0394e+00,  ...,  3.9600e-01,
          -2.6966e-01, -1.1513e-02]],
        ...,
        [[ 3.8979e+00,  1.6996e-01,  9.9212e-01,  ..., -1.9628e-01,
          -6.0056e-01,  7.5402e-02],
         [ 1.5429e+00,  1.7500e-01,  1.2568e+00,  ...,  3.4461e-02,
          -1.3941e-01, -1.8494e-01],
         [ 1.5546e+00,  1.7875e-01,  1.0410e+00,  ...,  7.1340e-02,
          -5.0805e-01, -2.0916e-01],
         ...,
         [ 1.1036e+00,  1.9328e-01,  1.1558e+00,  ...,  3.5664e-01,
          -2.6565e-01, -3.2526e-01],
         [ 1.2290e+00,  1.9410e-01,  1.1431e+00,  ...,  4.1280e-01,
          -3.1331e-01, -3.3246e-01],
         [ 1.1956e+00,  1.9343e-01,  1.0034e+00,  ...,  3.6756e-01,
          -2.1541e-01,  1.0444e-01]],
        [[ 3.9450e+00,  1.7066e-01,  1.3605e+00,  ..., -3.8384e-01,
          -7.5552e-01, -2.2509e-02],
         [ 1.5118e+00,  1.7784e-01,  1.1782e+00,  ..., -1.2112e-03,
          -3.7414e-01,  2.5648e-01],
         [ 1.3341e+00,  1.7647e-01,  1.2727e+00,  ..., -1.1386e-01,
          -2.8259e-01, -3.5615e-01],
         ...,
         [ 1.2316e+00,  1.9376e-01,  1.0175e+00,  ...,  2.4623e-01,
          -1.4074e-01, -5.0594e-02],
         [ 1.2228e+00,  1.9202e-01,  1.1944e+00,  ...,  6.1831e-01,
           9.6028e-03, -2.5826e-01],
         [ 1.1098e+00,  1.9329e-01,  1.1987e+00,  ...,  1.1217e-01,
          -2.8951e-01, -7.7061e-02]],
        [[ 4.1227e+00,  1.7113e-01,  9.8034e-01,  ..., -1.3293e-01,
          -1.0104e+00, -8.8493e-02],
         [ 1.6838e+00,  1.7630e-01,  1.2395e+00,  ...,  2.4525e-01,
           7.5585e-02,  5.8852e-02],
         [ 1.4974e+00,  1.7967e-01,  6.4531e-01,  ...,  6.2362e-02,
          -2.7098e-01, -2.6519e-01],
         ...,
         [ 1.2191e+00,  1.9343e-01,  7.9270e-01,  ...,  3.9461e-01,
          -2.9821e-01, -1.3005e-01],
         [ 1.2134e+00,  1.9348e-01,  9.2290e-01,  ...,  2.4274e-01,
          -3.0448e-01, -3.4296e-01],
         [ 1.1258e+00,  1.9339e-01,  8.9100e-01,  ...,  1.5275e-01,
          -3.2426e-01, -1.6535e-01]]], device='cuda:0',
       grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
MaskedLMOutput(loss=tensor([7.8408, 7.8479, 7.8500, 7.8495], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 4.4450,  0.2247,  1.2380,  ..., -0.3295, -0.6696, -0.3168],
         [ 1.7651,  0.2400,  1.3336,  ...,  0.0103, -0.5060, -0.1494],
         [ 1.5313,  0.2398,  1.3722,  ...,  0.2513, -0.1662, -1.0923],
         ...,
         [ 1.1998,  0.2571,  0.8731,  ...,  0.3645, -0.4171, -0.4924],
         [ 1.2391,  0.2580,  0.9711,  ...,  0.2767, -0.3911, -0.3395],
         [ 1.2426,  0.2572,  0.7470,  ...,  0.3569, -0.4075, -0.0271]],
        [[ 4.5612,  0.2221,  1.1812,  ..., -0.0609, -0.8763, -0.2881],
         [ 1.4291,  0.2361,  1.1533,  ...,  0.0821, -0.6578, -0.1962],
         [ 1.3410,  0.2444,  0.8810,  ...,  0.2668, -0.6238, -0.4503],
         ...,
         [ 1.4175,  0.2574,  0.8574,  ...,  0.4165, -0.6544, -0.4129],
         [ 1.2650,  0.2578,  1.0139,  ...,  0.3805, -0.5490, -0.3866],
         [ 1.2939,  0.2570,  1.1237,  ...,  0.4376, -0.4404, -0.2676]],
        [[ 4.5538,  0.2236,  1.2790,  ..., -0.2341, -0.7418, -0.1449],
         [ 1.6096,  0.2357,  1.2773,  ..., -0.4991, -0.4578, -0.5099],
         [ 1.7164,  0.2409,  0.6682,  ...,  0.2362, -0.6546, -0.3135],
         ...,
         [ 1.1883,  0.2575,  0.7310,  ...,  0.4501, -0.3211, -0.3356],
         [ 1.2087,  0.2573,  1.1578,  ...,  0.3192, -0.4196, -0.0922],
         [ 1.5136,  0.2599,  1.1644,  ...,  0.2396, -0.4733, -0.4195]],
        ...,
        [[ 4.3507,  0.2255,  0.9691,  ..., -0.2924, -0.6734, -0.3576],
         [ 1.6963,  0.2364,  0.8707,  ...,  0.2912, -0.8306, -0.2161],
         [ 2.2369,  0.2373,  1.1882,  ..., -0.1659, -0.7546, -0.7510],
         ...,
         [ 1.3944,  0.2614,  0.9606,  ...,  0.2176, -0.5490, -0.3100],
         [ 1.3934,  0.2598,  0.7940,  ...,  0.1430, -0.6098, -0.4582],
         [ 1.1232,  0.2597,  1.0209,  ...,  0.2450, -0.4089, -0.3841]],
        [[ 4.2779,  0.2266,  1.0752,  ..., -0.0651, -1.0131, -0.1890],
         [ 1.6580,  0.2406,  1.4186,  ..., -0.1901, -0.3598, -0.5415],
         [ 1.1755,  0.2353,  1.1919,  ...,  0.0224, -0.1494, -0.3917],
         ...,
         [ 1.3958,  0.2612,  0.7718,  ...,  0.2946, -0.2256, -0.2526],
         [ 1.2444,  0.2583,  0.9925,  ...,  0.2921, -0.2775, -0.2431],
         [ 1.2952,  0.2596,  1.0996,  ...,  0.5206, -0.3503, -0.3632]],
        [[ 4.4374,  0.2253,  0.9597,  ...,  0.0452, -0.8836, -0.1143],
         [ 1.6289,  0.2375,  1.3406,  ..., -0.0999, -0.7458, -0.3777],
         [ 1.9906,  0.2418,  0.8496,  ...,  0.1415, -0.5851, -0.2098],
         ...,
         [ 1.2025,  0.2602,  0.8837,  ...,  0.1677, -0.3358, -0.3394],
         [ 1.2126,  0.2548,  0.9281,  ...,  0.4274, -0.2162, -0.4594],
         [ 1.2180,  0.2573,  0.6364,  ...,  0.4569, -0.1675, -0.2825]]],
       device='cuda:0', grad_fn=<GatherBackward>), hidden_states=None, attentions=None)
MaskedLMOutput(loss=tensor([7.7311, 7.7328, 7.7326, 7.7288], device='cuda:0',
       grad_fn=<GatherBackward>), logits=tensor([[[ 4.7939,  0.2726,  1.2910,  ..., -0.1926, -0.8221, -0.2968],
         [ 1.4519,  0.3062,  1.0253,  ...,  0.3307, -0.3891, -0.1769],
         [ 1.5906,  0.3073,  0.8624,  ...,  0.3303, -0.3893, -0.5342],
         ...,
         [ 1.3850,  0.3247,  1.0726,  ...,  0.3231, -0.4125, -0.3816],
         [ 1.3147,  0.3228,  0.7353,  ...,  0.4828, -0.3308, -0.3361],
         [ 1.3399,  0.3250,  0.7855,  ...,  0.5122, -0.4590, -0.3861]],
        [[ 4.8807,  0.2717,  1.2769,  ..., -0.1148, -0.8904, -0.8737],
         [ 1.7218,  0.2983,  1.1938,  ...,  0.4372, -0.5761, -0.5320],
         [ 1.7136,  0.3015,  1.3148,  ...,  0.0068, -0.5998, -0.6239],
         ...,
         [ 1.1784,  0.3232,  0.9485,  ...,  0.6260, -0.3692, -0.7271],
         [ 1.1399,  0.3240,  0.9658,  ...,  0.4336, -0.4132, -0.3170],
         [ 1.3397,  0.3240,  0.8987,  ...,  0.2973, -0.2860, -0.6065]],
        [[ 4.7762,  0.2749,  1.3071,  ...,  0.0164, -0.8826, -0.5084],
         [ 1.4314,  0.2974,  1.4353,  ...,  0.1736, -0.2539, -0.2789],
         [ 1.4784,  0.3074,  1.1207,  ..., -0.0897, -0.5526, -0.6726],
         ...,
         [ 1.1734,  0.3228,  0.6767,  ...,  0.2688, -0.2791, -0.4948],
         [ 1.2707,  0.3233,  1.0449,  ...,  0.3834, -0.4520, -0.4873],
         [ 1.4674,  0.3232,  0.8296,  ...,  0.3694, -0.6290, -0.5803]],
        ...,
        [[ 4.8932,  0.2721,  1.0449,  ..., -0.2422, -0.8834, -0.3163],
         [ 1.8235,  0.3075,  1.2697,  ...,  0.1754, -0.3802, -0.1152],
         [ 1.9273,  0.3024,  1.2269,  ...,  0.0258, -0.7243, -0.5762],
         ...,
         [ 1.2832,  0.3242,  0.9468,  ...,  0.1636, -0.6104, -0.5528],
         [ 1.3686,  0.3225,  1.0419,  ...,  0.5734, -0.3753, -0.5184],
         [ 1.0971,  0.3228,  0.8224,  ...,  0.5028, -0.2412, -0.4320]],
        [[ 4.7487,  0.2790,  1.0845,  ..., -0.2682, -0.6408, -0.2920],
         [ 1.6516,  0.3013,  1.1455,  ...,  0.2001, -0.6446, -0.4168],
         [ 1.5164,  0.2985,  0.9471,  ...,  0.1757, -0.4581, -0.7720],
         ...,
         [ 1.1375,  0.3265,  0.8611,  ...,  0.4376, -0.5102, -0.4382],
         [ 1.1836,  0.3229,  0.9490,  ...,  0.5547, -0.4996, -0.4515],
         [ 1.3669,  0.3243,  0.7078,  ...,  0.4351, -0.2669, -0.3633]],
        [[ 4.9449,  0.2671,  1.0745,  ..., -0.2408, -0.6543, -0.4837],
         [ 1.7796,  0.2963,  1.0093,  ..., -0.2273, -0.7315, -0.1864],
         [ 1.5041,  0.3054,  1.2829,  ...,  0.3138, -0.5710, -0.3459],
         ...,
         [ 1.3043,  0.3219,  0.8036,  ...,  0.5139, -0.2579, -0.4655],
         [ 1.2955,  0.3241,  0.9047,  ...,  0.3772, -0.4758, -0.4174],
         [ 1.4440,  0.3253,  0.8092,  ...,  0.4395, -0.2681, -0.4405]]],
Epoch 0:   0%|                                                                                                                                                                                         | 6/42735 [00:15<31:16:55,  2.64s/it, loss=31.4]
Traceback (most recent call last):
  File "/home/mpg02/MBMP/nigel.hartman/git/ArabidopsisBERT/PlantBERT/05_pretrain.py", line 80, in <module>
    loss.sum().backward()
  File "/scratch/users/nigel.hartman/bin/miniconda3/envs/plantbert/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scratch/users/nigel.hartman/bin/miniconda3/envs/plantbert/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt